Metadata-Version: 2.1
Name: LASExplanation
Version: 0.0.1
Summary: This package is a brief wrap-up toolkit built based on 2 explanation packages: LIME and SHAP. The package contains 2 explainers: LIMEBAG and SHAP. It takes data and fitted models as input and returns explanations about feature importance ranks and/or weights. (etc. what attributes matter most within the prediction model).
Home-page: https://github.com/kpeng2019/LAS
Author: Kewen Peng
Author-email: kpeng@ncsu.edu
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3
Description-Content-Type: text/markdown
Requires-Dist: pandas
Requires-Dist: numpy
Requires-Dist: lime
Requires-Dist: shap
Requires-Dist: scikit-learn
Requires-Dist: setuptools
Requires-Dist: ipython

# LAS
This package is a brief wrap-up toolkit built based on 2 explanation packages: LIME and SHAP. The package contains 2 explainers: LIMEBAG and SHAP. It takes data and fitted models as input and returns explanations about feature importance ranks and/or weights. (etc. what attributes matter most within the prediction model).

## rq1.py
The demo runs LIMEBAG on a default dataset. It generates and presents explanations about feature importance ranks and weights for all testing data points. Can be called by LIMEBAG.demo1()

## rq2.py
The demo uses the explanations returned from LIMEBAG to run an effect size test. A summary of feature importance ranks and weights will be generated and presented as output. Can be called by LIMEBAG.demo2()


