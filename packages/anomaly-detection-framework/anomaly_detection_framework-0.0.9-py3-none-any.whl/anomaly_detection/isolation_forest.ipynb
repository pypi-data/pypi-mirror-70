{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_main = \"data/ml3yil.csv\"\n",
    "path_franchise = \"data/BayiIlkVeriGoderimTarihi.xlsx\"\n",
    "\n",
    "train_start = \"2016-06-01\"\n",
    "train_end = \"2019-12-31\"\n",
    "test_start = \"2020-01-01\"\n",
    "test_end = \"2020-03-19\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "#READ DATA\n",
    "def read_csv_excel(path):\n",
    "    if path.endswith('.csv'):\n",
    "        data = pd.read_csv(path, error_bad_lines=False,encoding = \"ISO-8859-1\",sep = ';', engine='python')\n",
    "        if (data.empty):\n",
    "            print ('CSV file is empty')\n",
    "        else:\n",
    "            data['Tarih'] = data['Tarih'].apply(lambda x: convert_str_to_date(x))\n",
    "    else:\n",
    "            data = pd.read_excel(path).rename(columns={'ilkverigonderimtarihi': 'd_r_start_date'})\n",
    "    return data\n",
    "\n",
    "def delete_nan_franchise(data):\n",
    "    nan = data.query(\"d_r_start_date == 'nan'\")\n",
    "    del_bayi = nan[\"BayiKodu\"].to_list()\n",
    "    for i in del_bayi:\n",
    "        data = data[data.BayiKodu != i]\n",
    "        #print(i,\"Deleted\")\n",
    "    return data,del_bayi\n",
    "        \n",
    "def delete_nan_main(data,del_bayi):\n",
    "    for i in del_bayi:\n",
    "        data = data[data.BayiKodu != i]\n",
    "        #print(i,\"Deleted\")\n",
    "    return data\n",
    "\n",
    "def merge_bayi_franchise(data,franchise_Iot_starts):\n",
    "    data = pd.merge(data, franchise_Iot_starts[['BayiKodu', 'd_r_start_date']], on='BayiKodu', how='left')\n",
    "    data = data.query(\"d_r_start_date <= Tarih\")\n",
    "    data = data[['BayiKodu','Tarih','KisiSayimi','BayiTipi','BAyiTuru','City','District','gpslocation','d_r_start_date','KS']].reset_index(drop=True)\n",
    "    data = data.sort_values(['BayiKodu', 'Tarih'], ascending=[True, True]).reset_index(drop=True)\n",
    "    \n",
    "    #data['weekday'] = data['Tarih'].apply(lambda x: x.weekday()) #0-6\n",
    "    data['isoweekday'] = data['Tarih'].apply(lambda x: x.isoweekday()) #1-7 \n",
    "    data['week_parts'] = data['isoweekday'].apply(lambda x: 1 if x in [1, 2, 3, 4, 5] else 0)\n",
    "    # Feature engineering with the date\n",
    "    data['year']=data.Tarih.dt.year \n",
    "    data['month']=data.Tarih.dt.month \n",
    "    data['day']=data.Tarih.dt.day\n",
    "    return data\n",
    "\n",
    "def convert_str_to_date(row):\n",
    "    if '-' in list(row):\n",
    "        return datetime.datetime.strptime(str(row)[0:10] , '%Y-%m-%d')\n",
    "    else:\n",
    "        row = '0' + row if len(row) == 9 else row\n",
    "        return datetime.datetime.strptime(\"-\".join([row[6:],row[3:5], row[0:2]]) , '%Y-%m-%d')\n",
    "    \n",
    "def split_dataset(data):\n",
    "    data.set_index('Tarih',inplace=True)\n",
    "    train_df, test_df = data[train_start:train_end],data[test_start:test_end]\n",
    "    train_df = train_df.query(\"KisiSayimi >50 and KisiSayimi <2500\")\n",
    "    return train_df, test_df \n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################    \n",
    "#REMOVE EMPTY VENDORS\n",
    "def pivot_table_for_missing(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.pivot(index='Tarih', columns='BayiKodu', values='KisiSayimi')\n",
    "    data = data[250:1288]\n",
    "    return data\n",
    "\n",
    "\n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing_Values', 1 : 'Percent_Missing'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        'Percent_Missing', ascending=False).round(1)\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "\n",
    "\n",
    "def handling_missing_values(missing_values,train_df):\n",
    "    result=missing_values.query(\"Missing_Values > 694\").reset_index()\n",
    "    del_bayi = result[\"BayiKodu\"].to_list()\n",
    "    #First data\n",
    "    for i in del_bayi:\n",
    "        train_df = train_df[train_df.BayiKodu != i]\n",
    "    return train_df,del_bayi\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "#TRAIN DATASETS\n",
    "def get_bayi_type_1(row):\n",
    "    if row['BayiTuru'] in ['TTM', 'TTM_Sube']:\n",
    "        bayi_tipi = row['BayiTipi'] if row['BayiTipi'] == 'Cadde' else 'AVM'\n",
    "        return 'ttt_and_ttm_' + bayi_tipi\n",
    "    else:\n",
    "        return row['BayiTuru']\n",
    "\n",
    "\n",
    "\n",
    "def ttt_and_ttm_sube_cadde_weekday_train(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube' \").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='Cadde' and week_parts==1\").reset_index(drop=True)  \n",
    "    return data\n",
    "\n",
    "def ttt_and_ttm_sube_cadde_weekend_train(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube'\").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='Cadde' and week_parts==0\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def avm_weekday_train(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube'\").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='AVM' or BayiTipi =='Avm' and week_parts==1\").reset_index(drop=True)\n",
    "    data = data.query(\"week_parts==1\")\n",
    "    return data\n",
    "\n",
    "def avm_weekend_train(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube'\").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='AVM' or BayiTipi =='Avm'\").reset_index(drop=True)\n",
    "    data = data.query(\"week_parts==0\")\n",
    "    return data\n",
    "\n",
    "def tt_il_hq_weekday_train(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TT_IL_HQ' and week_parts==1\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def tt_il_hq_weekend_train(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TT_IL_HQ' and week_parts==0\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "#TEST DATASETS\n",
    "def ttt_and_ttm_sube_cadde_weekday_test(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube'\").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='Cadde' and week_parts==1\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def ttt_and_ttm_sube_cadde_weekend_test(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube'\").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='Cadde' and week_parts==0\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def avm_weekday_test(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube' \").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='AVM' or BayiTipi =='Avm'\").reset_index(drop=True)\n",
    "    data = data.query(\"week_parts==1\")\n",
    "    return data\n",
    "\n",
    "def avm_weekend_test(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TTM' or BAyiTuru =='TTM_Sube' \").reset_index(drop=True)\n",
    "    data = data.query(\"BayiTipi =='AVM' or BayiTipi =='Avm'\").reset_index(drop=True)\n",
    "    data = data.query(\"week_parts==0\")\n",
    "    return data\n",
    "\n",
    "def tt_il_hq_weekday_test(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TT_IL_HQ' and week_parts==1\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def tt_il_hq_weekend_test(data):\n",
    "    data = data.reset_index()\n",
    "    data = data.query(\"BAyiTuru =='TT_IL_HQ' and week_parts==0\").reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "#LINEAR IMPUTATION FOR TRAIN DATA\n",
    "def fill_linear_imputation_train(data):\n",
    "    data = data.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "    return data\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "#FILL NAN FOR TEST DATA\n",
    "def fill_nan_test(data):\n",
    "    data = data.fillna(0)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "data_1 = read_csv_excel(path_main)\n",
    "\n",
    "data_2 = read_csv_excel(path_franchise)\n",
    "\n",
    "data_franch,del_bayi = delete_nan_franchise(data_2)\n",
    "\n",
    "data_main = delete_nan_main(data_1,del_bayi)\n",
    "\n",
    "df_new = merge_bayi_franchise(data_main,data_franch)\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "train_df, test_df = split_dataset(df_new)\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "df = pivot_table_for_missing(train_df)\n",
    "\n",
    "missing_values = missing_values_table(df)\n",
    "\n",
    "train_df,del_bayi = handling_missing_values(missing_values,train_df)\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "ttm_df_cadde_train_weekday = ttt_and_ttm_sube_cadde_weekday_train(train_df)\n",
    "\n",
    "ttt_and_ttm_sube_cadde_weekend = ttt_and_ttm_sube_cadde_weekend_train(train_df)\n",
    "\n",
    "ttm_df_avm_train_weekday = avm_weekday_train(train_df)\n",
    "\n",
    "ttm_df_avm_train_weekend = avm_weekend_train(train_df)\n",
    "\n",
    "tt_il_hq_df_train_weekday = tt_il_hq_weekday_train(train_df)\n",
    "\n",
    "tt_il_hq_df_train_weekend = tt_il_hq_weekend_train(train_df)\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "ttm_df_cadde_test_weekday = ttt_and_ttm_sube_cadde_weekday_test(test_df)\n",
    "\n",
    "ttm_df_cadde_test_weekend = ttt_and_ttm_sube_cadde_weekend_test(test_df)\n",
    "\n",
    "ttm_df_avm_test_weekday = avm_weekday_test(test_df)\n",
    "\n",
    "ttm_df_avm_test_weekend = avm_weekend_test(test_df)\n",
    "\n",
    "tt_il_hq_df_test_weekday = tt_il_hq_weekday_test(test_df)\n",
    "\n",
    "tt_il_hq_df_test_weekend = tt_il_hq_weekend_test(test_df)\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "ttm_df_cadde_train_weekday = fill_linear_imputation_train(ttm_df_cadde_train_weekday)\n",
    "\n",
    "ttt_and_ttm_sube_cadde_weekend = fill_linear_imputation_train(ttt_and_ttm_sube_cadde_weekend)\n",
    "\n",
    "ttm_df_avm_train_weekday = fill_linear_imputation_train(ttm_df_avm_train_weekday)\n",
    "\n",
    "ttm_df_avm_train_weekend = fill_linear_imputation_train(ttm_df_avm_train_weekend)\n",
    "\n",
    "tt_il_hq_df_train_weekday = fill_linear_imputation_train(tt_il_hq_df_train_weekday)\n",
    "\n",
    "tt_il_hq_df_train_weekend = fill_linear_imputation_train(tt_il_hq_df_train_weekend)\n",
    "\n",
    "######################################################################################################################\n",
    "######################################################################################################################\n",
    "\n",
    "ttm_df_cadde_test_weekday = fill_nan_test(ttm_df_cadde_test_weekday)\n",
    "\n",
    "ttm_df_cadde_test_weekend = fill_nan_test(ttm_df_cadde_test_weekend)\n",
    "\n",
    "ttm_df_avm_test_weekday = fill_nan_test(ttm_df_avm_test_weekday)\n",
    "\n",
    "ttm_df_avm_test_weekend = fill_nan_test(ttm_df_avm_test_weekend)\n",
    "\n",
    "tt_il_hq_df_test_weekday = fill_nan_test(tt_il_hq_df_test_weekday)\n",
    "\n",
    "tt_il_hq_df_test_weekend = fill_nan_test(tt_il_hq_df_test_weekend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_forest_train(df, num_of_trees,contamination,args):\n",
    "    \n",
    "    df.set_index('Tarih',inplace=True)\n",
    "    train = df[['KisiSayimi']].values\n",
    "    \n",
    "    it_model = IsolationForest(n_estimators=num_of_trees, max_samples='auto', contamination=contamination, \\\n",
    "                bootstrap=False, n_jobs=-1, random_state=42, verbose=1)\n",
    "    \n",
    "    it_model.fit(train)\n",
    "    \n",
    "    filename = 'models_week/'+args+'_iso_forest.sav'\n",
    "    pickle.dump(it_model, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "iso_forest_train(ttm_df_cadde_train_weekday, num_of_trees = 100, contamination=0.1,args=\"ttm_df_cadde_train_weekday\")\n",
    "\n",
    "iso_forest_train(ttt_and_ttm_sube_cadde_weekend, num_of_trees = 100, contamination=0.1,args=\"ttm_df_cadde_train_weekend\")\n",
    "\n",
    "iso_forest_train(ttm_df_avm_train_weekday, num_of_trees = 100, contamination=0.1,args=\"ttm_df_avm_train_weekday\")\n",
    "\n",
    "iso_forest_train(ttm_df_avm_train_weekend, num_of_trees = 100, contamination=0.1,args=\"ttm_df_avm_train_weekend\")\n",
    "\n",
    "iso_forest_train(tt_il_hq_df_train_weekday, num_of_trees = 100, contamination=0.1,args=\"TT_IL_HQ_df_train_weekday\")\n",
    "\n",
    "iso_forest_train(tt_il_hq_df_train_weekend, num_of_trees = 100, contamination=0.1,args=\"TT_IL_HQ_df_train_weekend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_predict(test_df,args):\n",
    "    \n",
    "    filename = \"models_week/\"+args\n",
    "    clf_load = pickle.load(open(filename, 'rb'))\n",
    "    \n",
    "    test = test_df[['KisiSayimi']].values\n",
    "    \n",
    "    pred = clf_load.predict(test)\n",
    "    scores = clf_load.score_samples(test)\n",
    "    \n",
    "    decision_scores = clf_load.decision_function(test)\n",
    "    original_paper_score = [-1*s + 0.5 for s in decision_scores]\n",
    "    original_paper_score = array(original_paper_score)\n",
    "    \n",
    "    #score_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #score_scaled = score_scaler.fit_transform(original_paper_score.reshape(-1, 1))\n",
    "    \n",
    "    test_df['anomaly'] = pred\n",
    "    test_df['scores'] = scores\n",
    "    test_df['dscores'] = decision_scores\n",
    "    test_df['ops'] = original_paper_score\n",
    "    \n",
    "    processed=0\n",
    "    processed = processed + 1\n",
    "    print(\"args\",args,processed)\n",
    "    \n",
    "    print(test_df['anomaly'].value_counts())\n",
    "    print(\"*\" * 100)\n",
    "   \n",
    "    return test_df, scores, decision_scores, original_paper_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args ttm_df_cadde_train_weekday_iso_forest.sav 1\n",
      " 1    23338\n",
      "-1     6721\n",
      "Name: anomaly, dtype: int64\n",
      "****************************************************************************************************\n",
      "args ttm_df_cadde_train_weekend_iso_forest.sav 1\n",
      " 1    9052\n",
      "-1    2546\n",
      "Name: anomaly, dtype: int64\n",
      "****************************************************************************************************\n",
      "args ttm_df_avm_train_weekday_iso_forest.sav 1\n",
      " 1    4543\n",
      "-1    1232\n",
      "Name: anomaly, dtype: int64\n",
      "****************************************************************************************************\n",
      "args ttm_df_avm_train_weekend_iso_forest.sav 1\n",
      " 1    1766\n",
      "-1     462\n",
      "Name: anomaly, dtype: int64\n",
      "****************************************************************************************************\n",
      "args TT_IL_HQ_df_train_weekday_iso_forest.sav 1\n",
      " 1    6296\n",
      "-1    2824\n",
      "Name: anomaly, dtype: int64\n",
      "****************************************************************************************************\n",
      "args TT_IL_HQ_df_train_weekend_iso_forest.sav 1\n",
      "-1    2855\n",
      " 1     665\n",
      "Name: anomaly, dtype: int64\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "result_df_ttm_cadde_weekday,scores,decision_scores,original_paper_score = iso_predict(ttm_df_cadde_test_weekday,args=\"ttm_df_cadde_train_weekday_iso_forest.sav\")\n",
    "\n",
    "result_df_ttm_cadde_weekend,scores,decision_scores,original_paper_score = iso_predict(ttm_df_cadde_test_weekend,args=\"ttm_df_cadde_train_weekend_iso_forest.sav\")\n",
    "\n",
    "ttm_df_avm_weekday,scores,decision_scores,original_paper_score = iso_predict(ttm_df_avm_test_weekday,args=\"ttm_df_avm_train_weekday_iso_forest.sav\")\n",
    "\n",
    "ttm_df_avm_weekend,scores,decision_scores,original_paper_score = iso_predict(ttm_df_avm_test_weekend,args=\"ttm_df_avm_train_weekend_iso_forest.sav\")\n",
    "\n",
    "tt_il_hq_df_test_weekday,scores,decision_scores,original_paper_score = iso_predict(tt_il_hq_df_test_weekday,args=\"TT_IL_HQ_df_train_weekday_iso_forest.sav\")\n",
    "\n",
    "tt_il_hq_df_test_weekend,scores,decision_scores,original_paper_score = iso_predict(tt_il_hq_df_test_weekend,args=\"TT_IL_HQ_df_train_weekend_iso_forest.sav\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
