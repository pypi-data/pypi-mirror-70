"""
BaselineLatent.py

Enhanced Integrated Gradients (EIG) allows users to use different type of baselines when computing attributions.
This class computes different types of baselines for EIG (zero, k-means, median, close and random) in the latent space
generated by an autoencoder. Latent space refers to last encoded representation that is input to the decoder of the
autoencoder.

1. encoded-zero: This baseline is an all zero data point in the latent space. Computing encoded-zero baseline does not
                    require a baseline class.

2. k-means: This baseline computes x clusters in the baseline class. Cluster centroids are returned as baseline points.
            Number of clusters (x) is supplied by the user.

3. median: This baseline returns points close to the median of the baseline class.

4. close: This baseline returns points from the baseline that are closest to the sample for which attributions are being
            generated.

5. random: This baseline returns random points from the baseline class.

One or more baseline points can be returned for: k-means, median, close and random.

Defines:
+ class BaselineLatent

"""

import numpy as np
from sklearn.cluster import KMeans

BASELINES_ENCODER_ZERO = "encoded_zero"
BASELINES_K_MEANS = "k-means"
BASELINES_MEDIAN = "median"
BASELINES_RANDOM = "random"
MAX_POINTS_FOR_K_MEANS = 20000


class BaselineLatent(object):
    """
    Class for baselines.

    Attributes
    ----------
    _k_means: np.array(shape=(number_of_baselines, number of features))
        numpy array with cluster centers as baselines
    _median: np.array(shape=(number_of_baselines, number of features))
        numpy array with number_of_baseline points closest according to L1 to median as baselines
    _random: np.array(shape=(number_of_baselines, number of features))
        numpy array with randomly chosen number_of_baseline points as baselines
    _encoder_zeros: np.array(shape=(number_of_baselines, number of features))
        numpy array with encoded-zero as baselines

    Methods
    -------
    _get_k_means_baselines(data: dict, no_baselines: int) -> np.array
        Obtains k_means_baseline as np.array
    _get_median_baselines(data: dict, no_baselines: int) -> np.array
        Obtains median baselines as np.array
    _get_random_baselines(data: dict, no_baselines: int) -> np.array
        Obtains random baselines as np.array
    get_baseline(data: dict, no_baselines: int, baseline_type: str) -> np.array
        Based on the baseline_type, return baseline array with no_of_baselines points
    """
    def __init__(self):
        self._encoder_zeros = []
        self._k_means = []
        self._median = []
        self._random = []

    def _get_encoder_zero_baselines(self, data, no_baselines, autoencoder=None):
        """
        Compute all encoded_zero baselines.
        :param data: np.array(shape=(number of events, number of features))
        :param no_baselines: int
        :param autoencoder: LatentModel object, if baselines are needed in the latent space
        :return: np.array(shape=(no_baselines, number of features)):
            encoded_zero baseline
        """
        assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
        data_model, data_encoder, data_decoder = data
        data_shape = list(data_model.shape)
        data_shape[0] = no_baselines
        data_shape = tuple(data_shape)
        encoder_data = [np.zeros(data_shape)]
        if data_encoder is not None:
            encoder_data.extend(data_encoder)
        tmp = autoencoder.encoder(encoder_data)
        decoder_data = [np.zeros(tmp.shape)]
        if data_decoder is not None:
            decoder_data.extend(data_decoder)
        # Pass zero in latent space through the decoder to get encoded_zero baseline
        self._encoder_zeros = autoencoder.decoder(decoder_data)

    def _get_k_means_baselines(self, data, no_baselines, autoencoder=None):
        """
        Compute k-means baselines.
        :param data: np.array(shape=(number of events, number of features))
        :param no_baselines: int
        :param autoencoder: LatentModel object, if baselines are needed in the latent space
        :return: np.array(shape=(no_baselines, number of features)):
            k-means baselines
        """
        assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
        data_model, data_encoder, data_decoder = data

        if len(data_model) > MAX_POINTS_FOR_K_MEANS:
            print("Too many data points for k-means, should be less than {}".format(MAX_POINTS_FOR_K_MEANS))
            exit(1)
        encoder_data = [data_model]
        if data_encoder is not None:
            encoder_data.extend(data_encoder)
        latent_data = autoencoder.encoder(encoder_data)
        # Run k-means with number of clusters as the number of points we are running.
        k_means = KMeans(n_clusters=no_baselines, random_state=0).fit(latent_data)

        decoder_data = [k_means.cluster_centers_]
        if data_decoder is not None:
            decoder_data.extend(data_decoder)
        self._k_means = autoencoder.decoder(decoder_data)

    def _get_median_baselines(self, data, no_baselines, autoencoder=None):
        """
        Compute median baselines.
        :param data: np.array(shape=(number of events, number of features))
        :param no_baselines: int
        :param autoencoder: LatentModel object, if baselines are needed in the latent space
        :return: np.array(shape=(no_baselines, number of features)):
            median baselines
        """
        assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
        data_model, data_encoder, data_decoder = data
        encoder_data = [data_model]
        if data_encoder is not None:
            encoder_data.extend(data_encoder)
        latent_data = autoencoder.encoder(encoder_data)

        # Find median points
        data_points_median = np.tile(np.median(latent_data, axis=0), (latent_data.shape[0], 1))
        values = np.apply_along_axis(lambda row: np.linalg.norm(row, ord=1), 1,
                                     latent_data - data_points_median)
        median_points = np.flip(np.argsort(values), axis=0)
        # Select points close to the median
        median_points = median_points[1:no_baselines + 1]
        latent_median = latent_data[median_points, :]
        decoder_data = [latent_median]
        if data_decoder is not None:
            decoder_data.extend(data_decoder)
        self._median = autoencoder.decoder(decoder_data)

    def _get_random_baselines(self, data, no_baselines, autoencoder=None):
        """
        Compute random baselines.
        :param data: np.array(shape=(number of events, number of features))
        :param no_baselines: int
        :param autoencoder: LatentModel object, if baselines are needed in the latent space
        :return: np.array(shape=(no_baselines, number of features)):
            random baselines
        """
        assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
        data_model, data_encoder, data_decoder = data
        encoder_data = [data_model]
        if data_encoder is not None:
            encoder_data.extend(data_encoder)
        latent_data = autoencoder.encoder(encoder_data)
        # Randomly choose baseline points
        idx = np.random.choice(np.arange(0, len(latent_data)), size=no_baselines)
        latent_random = latent_data[idx]
        decoder_data = [latent_random]
        if data_decoder is not None:
            decoder_data.extend(data_decoder)
        self._random = autoencoder.decoder(decoder_data)

    def get_baseline(self, data, no_baselines, baseline_type, autoencoder=None):
        """
        Based on the baseline_type, compute and return baselines.
        :param data: np.array(shape=(number of events, number of features))
        :param no_baselines: int
        :param baseline_type: str, which baseline class (zeros, k-means, median, random)
        :param autoencoder: LatentModel object, if baselines are needed in the latent space
        :return: np.array(shape=(no_baselines, number of features)):
            return baselines based on baseline_type
        """
        if baseline_type == BASELINES_ENCODER_ZERO:
            assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
            self._get_encoder_zero_baselines(data, no_baselines, autoencoder)
            return self._encoder_zeros
        if baseline_type == BASELINES_K_MEANS:
            assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
            self._get_k_means_baselines(data, no_baselines, autoencoder=autoencoder)
            return self._k_means
        if baseline_type == BASELINES_MEDIAN:
            assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
            self._get_median_baselines(data, no_baselines, autoencoder=autoencoder)
            return self._median
        if baseline_type == BASELINES_RANDOM:
            assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
            self._get_random_baselines(data, no_baselines, autoencoder=autoencoder)
            return self._random

    @staticmethod
    def get_closest_baselines(data, no_baselines, samples, autoencoder=None):
        """
        Compute closest points of opposite class as baselines.
        :param data: np.array(shape=(number of events, number of features))
        :param no_baselines: int
        :param samples: np.array(shape=(number of events, number of features))
        :param autoencoder: LatentModel object, if baselines are needed in the latent space
        :return: np.array(shape=(no_baselines, number of features)): median baselines, list: baseline_ids
        """
        assert autoencoder is not None, "Pass an autoencoder (LatentModel Object)"
        data_model, data_encoder, data_decoder = data
        samples_model, samples_encoder, samples_decoder = samples
        encoder_data = [data_model]
        if data_encoder is not None:
            encoder_data.extend(data_encoder)
        latent_data = autoencoder.encoder(encoder_data)
        encoder_samples = [samples_model]
        if samples_encoder is not None:
            encoder_samples.extend(samples_encoder)
        latent_samples = autoencoder.encoder(encoder_samples)

        cnt = 0
        all_baselines_encoded = []
        all_baseline_ids_encoded = []
        for ii in latent_samples:
            if cnt % 50 == 0:
                print(cnt)
            # Find points that are in the baseline class but close to the sample
            baseline = np.tile(ii, (latent_data.shape[0], 1))
            values = np.apply_along_axis(lambda row: np.linalg.norm(row, ord=1), 1, latent_data - baseline)
            close_points = np.argsort(values)
            # Select those as baseline
            close_points = close_points[1:no_baselines + 1]
            points = latent_data[close_points, :]
            all_baselines_encoded.extend(points)
            all_baseline_ids_encoded.append(cnt)
            cnt += 1

        decoder_data = [all_baselines_encoded]
        if data_decoder is not None:
            decoder_data.extend(data_decoder)
        all_baselines = autoencoder.decoder(decoder_data)
        all_baselines = np.array(all_baselines)
        all_baseline_ids = np.array(all_baseline_ids_encoded)
        return all_baselines, all_baseline_ids
