# neural-texturize â€” Copyright (c) 2020, Novelty Factory KG.  See LICENSE for details.

import itertools

import torch
import torch.nn.functional as F


class GramMatrixCritic:
    """A `Critic` evaluates the features of an image to determine how it scores.

    This critic computes a 2D histogram of feature cross-correlations for the specified
    layer (e.g. "1_1") or layer pair (e.g. "1_1:2_1"), and compares it to the target
    gram matrix.
    """

    def __init__(self, layer, offset: float = -1.0):
        self.pair = tuple(layer.split(":"))
        if len(self.pair) == 1:
            self.pair = (self.pair[0], self.pair[0])
        self.offset = offset
        self.gram = None

    def evaluate(self, features):
        current = self._prepare_gram(features)
        yield 1e4 * F.mse_loss(current, self.gram.expand_as(current), reduction="mean")

    def from_features(self, features):
        self.gram = self._prepare_gram(features)

    def get_layers(self):
        return set(self.pair)

    def _gram_matrix(self, column, row):
        (b, ch, h, w) = column.size()
        f_c = column.view(b, ch, w * h)
        (b, ch, h, w) = row.size()
        f_r = row.view(b, ch, w * h)

        gram = (f_c / w).bmm((f_r / h).transpose(1, 2)) / ch
        assert not torch.isnan(gram).any()

        return gram

    def _prepare_gram(self, features):
        lower = features[self.pair[0]] + self.offset
        upper = features[self.pair[1]] + self.offset
        return self._gram_matrix(
            lower, F.interpolate(upper, size=lower.shape[2:], mode="nearest")
        )


class PatchBuilder:
    def __init__(self, patch_size=3, weights=None):
        self.min = -((patch_size - 1) // 2)
        self.max = patch_size + self.min - 1
        self.patch_size = patch_size

        if weights is None:
            weights = torch.ones(size=(patch_size ** 2,))
        else:
            weights = torch.tensor(weights, dtype=torch.float32)

        self.weights = weights / weights.sum()

    def extract(self, array):
        padded = F.pad(
            array,
            pad=(abs(self.min), self.max, abs(self.min), self.max),
            mode="replicate",
        )
        h, w = (
            padded.shape[2] - self.patch_size + 1,
            padded.shape[3] - self.patch_size + 1,
        )
        output = []
        for i, (y, x) in enumerate(itertools.product(self.coords, repeat=2)):
            p = padded[:, :, y : h + y, x : w + x] * self.weights[i]
            output.append(p)
        return torch.cat(output, dim=1)

    @property
    def coords(self):
        return range(self.patch_size)


def cosine_similarity_matrix_1d(source, target, eps=1e-8):
    source = source / (torch.norm(source, dim=1, keepdim=True) + eps)
    target = target / (torch.norm(target, dim=1, keepdim=True) + eps)

    result = torch.bmm(source.permute(0, 2, 1), target)
    return torch.clamp(result, max=1.0 / eps)


def nearest_neighbors_1d(_a, _b, split=1, eps=1e-8):
    a = _a.flatten(2)
    b = _b.flatten(2)

    batch = a.shape[0]
    size = b.shape[2] // split

    score_a = a.new_full((batch, a.shape[2]), float("-inf"))
    index_a = a.new_full((batch, a.shape[2]), -1, dtype=torch.int64)
    score_b = b.new_full((batch, b.shape[2]), float("-inf"))
    index_b = b.new_full((batch, b.shape[2]), -1, dtype=torch.int64)

    for i in range(split):
        start_b, finish_b = i * size, (i + 1) * size
        bb = b[:, :, start_b:finish_b]
        sim = cosine_similarity_matrix_1d(a, bb, eps=eps)

        max_a = torch.max(sim, dim=2)
        cond_a = max_a.values > score_a
        index_a[:] = torch.where(cond_a, max_a.indices + start_b, index_a)
        score_a[:] = torch.where(cond_a, max_a.values, score_a)

        max_b = torch.max(sim, dim=1)
        slice_b = slice(start_b, finish_b)
        cond_b = max_b.values > score_b[:, slice_b]
        index_b[:, slice_b] = torch.where(cond_b, max_b.indices, index_b[:, slice_b])
        score_b[:, slice_b] = torch.where(cond_b, max_b.values, score_b[:, slice_b])

    return index_a, score_a, index_b, score_b


def cosine_similarity_vector_1d(source, target, eps=1e-8):
    source = source / (torch.norm(source, dim=1, keepdim=True) + eps)
    target = target / (torch.norm(target, dim=1, keepdim=True) + eps)

    # target = target.unsqueeze(-1).expand(*target.shape, 5)
    # source = source.unsqueeze(-1).expand_as(target)

    result = torch.sum(source * target, dim=1)
    # assert (result[:, :, 0] == result[:, :, 1]).all()
    return torch.clamp(result, max=1.0 / eps)


"""
CLUSTERING

def knn_1d(a, b, split=1, eps=1e-8):
    batch = a.shape[0]
    k = 4
    size = a.shape[2] // split

    score_a = a.new_full((batch, a.shape[2], k), float("-inf"))
    index_a = a.new_full((batch, a.shape[2], k), -1, dtype=torch.int64)

    for i in range(split):
        start_a, finish_a = i * size, (i + 1) * size
        aa = a[:, :, start_a:finish_a]
        sim = cosine_similarity_1d(aa, b, eps=eps)

        max_b = torch.sort(sim, dim=2, descending=True)
        score_a[:, start_a:finish_a] = max_b.values[:, :, 1:5]
        index_a[:, start_a:finish_a] = max_b.indices[:, :, 1:5]

        # print(max_b.values.shape, aa.shape, b.shape)
        # print(max_b.values[:, 0, 0], max_b.values[:, 0, 2])

        # cond_a = max_a.values > score_a
        # index_a[:] = torch.where(cond_a, max_a.indices, index_a)
        # score_a[:] = torch.where(cond_a, max_a.values, score_a)

    return index_a
"""


class PatchCritic:
    def __init__(self, layer):
        self.layer = layer
        self.patches = None
        self.builder = PatchBuilder(
            patch_size=2, weights=[0.3, 0.1, 0.1, 0.1]
            # [0.1, 0.1, 0.1, 0.1, 0.8, 0.1, 0.1, 0.1, 0.1]
        )
        self.split_hints = {}

        self.index_a = None
        self.index_b = None

    def get_layers(self):
        return {self.layer}

    def from_features(self, features):
        self.patches = self.prepare(features).detach()
        return

        u, s, v = torch.svd(self.patches.permute(0, 2, 1), some=True)
        m = torch.bmm(u, torch.diagflat(s.squeeze(0)).unsqueeze(0))
        # print('RESULT', m.shape, 'TRANSFORM', v.shape)

        self.patches = m.permute(0, 2, 1)
        self.proj = v

        # m[:, :, 128:] = 0.0
        # repro = torch.bmm(m, v.permute(0, 2, 1))
        # print(repro.shape, patches.shape)
        # e = F.mse_loss(patches, repro)
        # print(e.item())

        # recomp = torch.bmm(patches, v)
        # e = F.mse_loss(recomp, m)
        # print(e.item())

        # print(m.shape)
        # p2 = torch.mm(patches, m)

        # patches = self.patches.flatten(2)
        # ids = knn_1d(patches, patches, split=64)
        # print(ids[:,0], ids[:,1])
        # print(ids.shape)

    def prepare(self, features):
        f = features[self.layer]
        p = self.builder.extract(f)  # .flatten(2)
        return p

    def evaluate(self, features):
        current = self.prepare(features)
        # current = torch.bmm(current.permute(0, 2, 1), self.proj).permute(0, 2, 1)
        yield from self.bidirectional_patch_similarity(current, self.patches)

    def bidirectional_patch_similarity(self, source, target, eps=1e-8):
        assert source.shape[0] == 1 and target.shape[0] == 1
        assert source.shape[1] == target.shape[1]

        with torch.no_grad():
            if source.shape[2] * source.shape[3] <= 128 ** 2: 
                for i in self.split_hints.get(source.shape[1:], range(16)):
                    try:
                        k = source.shape[1] # // 4
                        ids, scs, idt, sct = nearest_neighbors_1d(
                            source[:, :k], target[:, :k], split=2 ** i, eps=eps
                        )
                        self.split_hints[source.shape[1:]] = [i]

                        self.index_a = ids.view(1, *source.shape[2:])
                        self.index_b = idt.view(1, *target.shape[2:])

                        break
                    except RuntimeError:
                        continue
            else:
                for i in self.split_hints.get(source.shape[1:], range(6, 0, -1)):
                    try:
                        ids, scs, idt, sct = self.approximate_nearest_neighbors_1d(
                            source, target, batch=2 ** i, eps=eps
                        )
                        self.split_hints[source.shape[1:]] = [i]
                    except RuntimeError:
                        continue

            matched_source = torch.index_select(target.flatten(2), dim=2, index=ids[0])

        yield 0.5 * F.mse_loss(source.flatten(2), matched_source)

        matched_target = torch.index_select(source.flatten(2), dim=2, index=idt[0])
        yield 0.5 * F.mse_loss(target.flatten(2), matched_target)

    def approximate_nearest_neighbors_1d(self, _a, _b, batch=16, eps=1e-8):
        a = _a.flatten(2)
        b = _b.flatten(2)

        score_a = a.new_full((1, a.shape[2]), float("-inf"))
        index_a = a.new_full((1, a.shape[2]), -1, dtype=torch.int64)

        for i in range(5):
            bids = torch.randint(
                    low=0,
                    high=b.shape[2],
                    size=(a.shape[2], batch), device=b.device)

            if i == 0 and self.index_a is not None:

                if self.index_a.shape[1:] != _a.shape[2:]:
                    print('UPSCALE', _a.shape[2:])

                    h, w = self.index_a.shape[1:]
                    y = self.index_a // w
                    x = self.index_a % w
                    new_idx = (y * 2) * _a.shape[3] + (x * 2)
                    self.index_a = F.interpolate(new_idx.unsqueeze(0).float(), size=_a.shape[2:]).squeeze(0).long()

                bids[:, 0] = self.index_a.flatten(1)

            bb = torch.index_select(b, dim=2, index=bids.flatten())
            bb = bb.view(*a.shape, batch)

            sim = cosine_similarity_vector_1d(a.unsqueeze(-1).expand_as(bb), bb, eps=eps)
            max_a = sim.max(dim=2)

            wins = torch.gather(bids, dim=1, index=max_a.indices.view(-1, 1))
            # print(max_a.indices.flatten().shape, 'x', bids.shape, "->", wins.shape)

            cond_a = max_a.values > score_a
            # print(max_a.values.shape, score_a.shape, '->', cond_a.shape, 'x', wins.shape)

            index_a[:] = torch.where(cond_a, wins.view(*index_a.shape), index_a)
            score_a[:] = torch.where(cond_a, max_a.values, score_a)

        # ======== 8< ========

        score_b = b.new_full((1, b.shape[2]), float("-inf"))
        index_b = b.new_full((1, b.shape[2]), -1, dtype=torch.int64)

        for i in range(5):
            aids = torch.randint(
                    low=0,
                    high=a.shape[2],
                    size=(b.shape[2], batch), device=a.device)

            if i == 0 and self.index_b is not None:

                if self.index_b.shape[1:] != _b.shape[2:]:
                    print('UPSCALE', _b.shape[2:])

                    h, w = self.index_b.shape[1:]
                    y = self.index_b // w
                    x = self.index_b % w
                    new_idx = (y * 2) * _b.shape[3] + (x * 2)
                    self.index_b = F.interpolate(new_idx.unsqueeze(0).float(), size=_b.shape[2:]).squeeze(0).long()

                aids[:, 0] = self.index_b.flatten(1)

            aa = torch.index_select(a, dim=2, index=aids.flatten())
            aa = aa.view(*b.shape, batch)
            
            sim = cosine_similarity_vector_1d(b.unsqueeze(-1).expand_as(aa), aa, eps=eps)
            max_b = sim.max(dim=2)

            wins = torch.gather(aids, dim=1, index=max_b.indices.view(-1, 1))
            # print(max_a.indices.flatten().shape, 'x', bids.shape, "->", wins.shape)

            cond_b = max_b.values > score_b
            index_b[:] = torch.where(cond_b, wins.view(*index_b.shape), index_b)
            score_b[:] = torch.where(cond_b, max_b.values, score_b)


        self.index_a = index_a.view(1, *_a.shape[2:])
        self.index_b = index_b.view(1, *_b.shape[2:])

        return index_a, score_a, index_b, score_b
