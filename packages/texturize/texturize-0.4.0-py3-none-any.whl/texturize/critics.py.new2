# neural-texturize â€” Copyright (c) 2020, Novelty Factory KG.  See LICENSE for details.

import itertools

import torch
import torch.nn.functional as F


class GramMatrixCritic:
    """A `Critic` evaluates the features of an image to determine how it scores.

    This critic computes a 2D histogram of feature cross-correlations for the specified
    layer (e.g. "1_1") or layer pair (e.g. "1_1:2_1"), and compares it to the target
    gram matrix.
    """

    def __init__(self, layer, offset: float = -1.0):
        self.pair = tuple(layer.split(":"))
        if len(self.pair) == 1:
            self.pair = (self.pair[0], self.pair[0])
        self.offset = offset
        self.gram = None

    def evaluate(self, features):
        current = self._prepare_gram(features)
        yield 1e4 * F.mse_loss(current, self.gram.expand_as(current), reduction="mean")

    def from_features(self, features):
        self.gram = self._prepare_gram(features)

    def get_layers(self):
        return set(self.pair)

    def _gram_matrix(self, column, row):
        (b, ch, h, w) = column.size()
        f_c = column.view(b, ch, w * h)
        (b, ch, h, w) = row.size()
        f_r = row.view(b, ch, w * h)

        gram = (f_c / w).bmm((f_r / h).transpose(1, 2)) / ch
        assert not torch.isnan(gram).any()

        return gram

    def _prepare_gram(self, features):
        lower = features[self.pair[0]] + self.offset
        upper = features[self.pair[1]] + self.offset
        return self._gram_matrix(
            lower, F.interpolate(upper, size=lower.shape[2:], mode="nearest")
        )


class PatchBuilder:
    def __init__(self, patch_size=3, weights=None):
        self.min = -((patch_size - 1) // 2)
        self.max = patch_size + self.min - 1
        self.patch_size = patch_size

        if weights is None:
            weights = torch.ones(size=(patch_size ** 2,))
        else:
            weights = torch.tensor(weights, dtype=torch.float32)

        self.weights = weights / weights.sum()

    def extract(self, array):
        padded = F.pad(
            array,
            pad=(abs(self.min), self.max, abs(self.min), self.max),
            mode="replicate",
        )
        h, w = (
            padded.shape[2] - self.patch_size + 1,
            padded.shape[3] - self.patch_size + 1,
        )
        output = []
        for i, (y, x) in enumerate(itertools.product(self.coords, repeat=2)):
            p = padded[:, :, y : h + y, x : w + x] # * self.weights[i]
            output.append(p)
        return torch.cat(output, dim=1)

    @property
    def coords(self):
        return range(self.patch_size)


def cosine_similarity_1d(source, target, eps=1e-8):
    source = source / (torch.norm(source, dim=1, keepdim=True) + eps)
    target = target / (torch.norm(target, dim=1, keepdim=True) + eps)

    result = torch.bmm(source.permute(0, 2, 1), target)
    return torch.clamp(result, max=1.0 / eps)


def nearest_neighbors_1d(a, b, split=1, eps=1e-8):
    batch = a.shape[0]
    size = b.shape[2] // split

    score_a = a.new_full((batch, a.shape[2]), float("-inf"))
    index_a = a.new_full((batch, a.shape[2]), -1, dtype=torch.int64)
    score_b = b.new_full((batch, b.shape[2]), float("-inf"))
    index_b = b.new_full((batch, b.shape[2]), -1, dtype=torch.int64)

    for i in range(split):
        start_b, finish_b = i * size, (i + 1) * size
        bb = b[:, :, start_b:finish_b]
        sim = cosine_similarity_1d(a, bb, eps=eps)

        max_a = torch.max(sim, dim=2)
        cond_a = max_a.values > score_a
        index_a[:] = torch.where(cond_a, max_a.indices + start_b, index_a)
        score_a[:] = torch.where(cond_a, max_a.values, score_a)

        max_b = torch.max(sim, dim=1)
        slice_b = slice(start_b, finish_b)
        cond_b = max_b.values > score_b[:, slice_b]
        index_b[:, slice_b] = torch.where(cond_b, max_b.indices, index_b[:, slice_b])
        score_b[:, slice_b] = torch.where(cond_b, max_b.values, score_b[:, slice_b])

    return index_a, index_b


from .match import FeatureMatcher

import os
import math
import pickle
import random
import collections

class Prediction:
    def __init__(self):
        self.total = 0.0
        self.count = 0

    def value(self, default=None):
        if self.count == 0:
            return default or random.uniform(0.0, 1.0)

        return self.total / self.count
    
    def update(self, value):
        self.total += value
        self.count += 1


class Policy:

    def __init__(self, actions):
        self.actions = set(actions)
        self.predictions = collections.defaultdict(Prediction)

    def predict(self, state, action):
        default = self.predictions[(None, action)].value()
        default = sum(self.predictions[(k, action)].value(default) for k in state) / len(state)
        return self.predictions[(state, action)].value(default)

    def sample(self, state, temperature=0.9):
        choices = {}
        for action in self.actions:
            choices[action] = self.predict(state, action) * random.uniform(temperature, 1.0)

        if random.uniform(0.0, 1.0) <= temperature:
            return max(choices, key=lambda k: choices[k])
        else:
            return random.choice(list(choices.keys()))

    def update(self, state, action, reward):
        self.predictions[(state, action)].update(reward)
        for k in state:
            self.predictions[(k, action)].update(reward)
        self.predictions[(None, action)].update(reward)


if not os.path.exists("policy.pkl"):
    POLICY = Policy(actions=itertools.product(['nearby', 'random'], [1, 2, 4, 8, 16]))
else:
    print('RELOADING!')
    POLICY = pickle.load(open('policy.pkl', 'rb'))


class PatchCritic:
    def __init__(self, layer):
        self.layer = layer
        self.patches = None

        self.builder = PatchBuilder(patch_size=2, weights=[0.3, 0.1, 0.1, 0.1])
        self.matcher = FeatureMatcher(device="cpu")

        self.split_hints = {}

    @classmethod
    def save(self):
        pickle.dump(POLICY, open('policy.pkl', 'wb'))

    def get_layers(self):
        return {self.layer}

    def from_features(self, features):
        self.patches = self.prepare(features).detach()
        self.matcher.update_sources(self.patches)
        self.iter = 0

    def prepare(self, features):
        f = features[self.layer]
        return self.builder.extract(f)

    def auto_split(self, function, *arguments, **keywords):
        key = (self.matcher.target.shape, function)
        for i in self.split_hints.get(key, range(16)):
            try:
                result = function(*arguments, split=2**i, **keywords)
                self.split_hints[key] = [i]
                return result
                break
            except RuntimeError as e:
                if "CUDA out of memory." not in str(e):
                    raise

    def learn_by_reinforcement(self, traces=256, steps=8, _matcher=None, greedy=False):
        ref1 = self.matcher.repro_target.scores.mean().item()
        ref2 = self.matcher.repro_sources.scores.mean().item()

        best = 0.0, None
        total = 0.0
        for _ in range(traces):
            matcher = _matcher or self.matcher.clone()

            def eval(fn, arg):
                changes = self.auto_split(getattr(matcher, 'compare_features_'+fn), arg)
                improve = max(0.0, matcher.repro_target.scores.mean().item() - ref1) \
                        + max(0.0, matcher.repro_sources.scores.mean().item() - ref2)
                return changes, improve

            trace, previous = [], 'upscaled'
            for k in range(8):
                it = int(math.log(self.iter+1)*2.307)
                state = (it, previous, k)
                action = POLICY.sample(state, temperature=1.0 if greedy else 0.9)

                trace.append((state, action))
                changes, improve = eval(*action)
                previous = action[0]

            new1 = matcher.repro_target.scores.mean().item()
            new2 = matcher.repro_sources.scores.mean().item()

            sc1 = max(new1 - ref1, 0.0)
            sc2 = max(new2 - ref2, 0.0)

            if traces > 1:
                for k, (state, action) in enumerate(trace):
                    POLICY.update(state, action, sc1 + sc2)

            total += sc1 + sc2
            if sc1 + sc2 > best[0]:
                best = (sc1 + sc2, [t[1] for t in trace])

            del matcher

            import gc; gc.collect()
            torch.cuda.empty_cache()

        # print('random', self.iter, k, '==', r * 100 / (target.shape[2] * target.shape[3]))
        if traces > 1:
            print('AVERAGE POLICY', self.iter, total / traces)
            print('SEARCH POLICY', self.iter, best)
        if greedy:
            print('GREEDY POLICY', self.iter, best)
        return best[1]

    def evaluate(self, features):
        self.iter += 1

        target = self.prepare(features)
        self.matcher.update_target(target)

        with torch.no_grad():
            self.matcher.repro_target.scores.fill_(float("-inf"))
            self.matcher.repro_sources.scores.fill_(float("-inf"))

            if self.iter == 1 or target.flatten(1).shape[1] < 802_816:
                self.auto_split(self.matcher.compare_features_matrix)
            else:
                self.auto_split(self.matcher.compare_features_identity)

                # self.learn_by_reinforcement(traces=64)
                self.learn_by_reinforcement(traces=1, _matcher=self.matcher, greedy=True)

            matched_target = self.matcher.reconstruct_target()

        assert matched_target.shape == target.shape
        yield 0.5 * F.mse_loss(target, matched_target)

        matched_source = self.matcher.reconstruct_source()
        assert matched_source.shape == self.patches.shape
        yield 0.5 * F.mse_loss(matched_source, self.patches)
