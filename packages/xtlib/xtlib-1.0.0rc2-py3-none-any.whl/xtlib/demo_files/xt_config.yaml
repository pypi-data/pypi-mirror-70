#  local xt_config.yaml file for cmdlineTest directory

#external-services:
    #xtcontainerregistry: {type: "registry", login-server: "xtcontainerregistry.azurecr.io", username: "xtcontainerregistry", password: "$vault", login: "true"}

# TEMP cluaster=cam
compute-targets: {}

setups:
    #local-all: {activate: "conda activate exper38", conda-packages: [], pip-packages: ["torch==1.2.0", "torchvision==0.4.1", "Pillow==6.2.0", "watchdog==0.9.0", "xtlib==*"] }
    py36: {activate: "$call conda activate py36", conda-packages: [], pip-packages: ["xtlib==*"]}
    aml: {pip-packages: ["torch==1.2.0", "torchvision==0.4.1", "Pillow==6.2.0", "watchdog==0.9.0", "horovod", "xtlib==*"] }

general:
    advanced-mode: true                # enable all XT commands
    workspace: "xt-demo"               # name of current workspace 
    experiment: "exper5"               # default name of experiment associated with each run
    xt-team-name: "sandbox"            # for use with XT Grok
    #monitor: "none"                   # should new run be monitored in console?  one of: new, same, none

    # metrics
    primary-metric: "test-acc"         # name of metric to optimize in roll-ups, hyperparameter search, and early stopping
    maximize-metric: true              # how primary metric is aggregated for hp search, hp explorer, early stopping 
    step-name: "epoch"                 # name of a single step
    remote-control: true               # talk to the XT controller from XT client

logging:
    mirror-files: "output/logs/**"     # default wildcard path for log files to mirror
    mirror-dest: "none"                # one of: none, storage

internal:
    stack-trace: true    
    pip-freeze: true                   # should 'pip freeze' be run during node setup process (logging before/after pip packages)

code:
    xtlib-upload: true                   # upload XTLIB sources files for each run and use for controller and ML app
    code-zip: "compress"                 # none/fast/compress ("fast" means zip w/o compression)
    code-omit: [".git", "__pycache__", "logs", "data"]      # directories and files to omit when capturing before/after files
    #working-dir: "code"                  # where to run relative to code directory

after-files:
    after-dirs: ["output/**", "logs/**"]   # specifies output files (for capture from compute node to STORE)
    after-upload: true                     # should after files be uploaded at end of run?
    after-omit: ["*.junk"]                 # directories and files to omit when capturing after files

data:
    # note: XT_DATA_DIR on cloud, pool, or local will always point to .../data/mnist

    data-share-path: "mnist"               # path to data in data share 
    data-local: "$scriptdir/../data"       # local directory of data for app
    data-action: "mount"                   # data action at start of run: none, download, mount
    data-writable : false

model:
    # note: XT_MODEL_DIR on cloud, pool, or local will always point to .../models/miniMnist

    model-local: "$scriptdir/../models"    # local directory of model for app
    model-share-path: "miniMnist"          # path to model in model share
    model-action: "mount"                  # model action at start of run: none, download, mount
    model-writable: true

hyperparameter-search:
    fn-generated-config: "runset.yaml"     # name of runset file generated by dynamic hyperparameter search

hyperparameter-explorer:
    steps-name: "epochs"                   # name of metric that represents total # of steps taken 
    log-interval-name: "log-interval"      # name of hyperparameter that specifies # of steps between metric reports

run-reports:
    uppercase-hdr: false
    columns: ["run", "job", 
        "created:$to", "experiment", "box", "queued", "target", "runs",  "search", "search_style=style", "status", 
        "tags.priority", "tags.description", "tags.top5", "tags.good_run",
        "hparams.lr", "hparams.momentum", "hparams.optimizer",
        
        "metrics.*",
        # "metrics.step", "hparams.steps",  "metrics.epoch", "hparams.epochs", "hparams.seed",
        # "metrics.train-loss", "metrics.dev-loss", "metrics.test-loss", 
        # "metrics.train-acc",  "metrics.dev-acc",  "metrics.test-acc", 

        "duration", 
        ]

job-reports:
    columns: ["job", "created", "started", "workspace", "experiment", "target", "nodes", "runs", "repeat", "tags.description", "tags.urgent", "tags.sad=SADD", "tags.funny", "low_pri", 
        "vm_size", "azure_image", "service", "vc", "cluster", "queue", "service_type", "search", "search_style",
        "job_status:$bz", "running_nodes:$bz", "running_runs:$bz", "error_runs:$bz", "completed_runs:$bz"]

boxes:
    # This section lets you define remote computers for running your experiments (samples listed below).
    # REQUIREMENTS: each box needs to have ports 22 and 18861 open for incoming messages.
    # The "actions" property is a list of store names ("data", "model") whose download or mount actions should be performed on the box.
    local: {address: "localhost", os: "windows", box-class: "windows", max-runs: 1, actions: [], setup: local}

#don't release as uncommented (will cause "no extensions module" error for XT DEMO users)
# providers:
#     command: {
#         "mycmds": "extensions.user_commands.ImplMyCommands" 
#     }
